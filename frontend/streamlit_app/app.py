import os, time
import streamlit as st
from dotenv import find_dotenv, load_dotenv

load_dotenv(find_dotenv())


# --- App Insights(OpenTelemetry) ---
from azure.monitor.opentelemetry import configure_azure_monitor
if os.getenv("APPLICATIONINSIGHTS_CONNECTION_STRING"):
    configure_azure_monitor(connection_string=os.getenv("APPLICATIONINSIGHTS_CONNECTION_STRING"))
from opentelemetry import trace
tracer = trace.get_tracer("streamlit_qa")

# --- LangFuse / LangSmith (optional) ---
from contextlib import suppress
lf, ls_client = None, None
with suppress(Exception):
    from langfuse import Langfuse
    if os.getenv("LANGFUSE_PUBLIC_KEY"):
        lf = Langfuse()
with suppress(Exception):
    from langsmith import Client as LSClient
    if os.getenv("LANGSMITH_API_KEY"):
        ls_client = LSClient()

# --- LLM / Retriever ---
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores.azuresearch import AzureSearch
from langchain.chains import RetrievalQA

llm = AzureChatOpenAI(
    azure_deployment=os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT"],
    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
    api_key=os.environ["AZURE_OPENAI_API_KEY"],
    api_version=os.environ["AZURE_OPENAI_API_VERSION"],
    temperature=0,
)

emb = AzureOpenAIEmbeddings(
    azure_deployment=os.environ["AZURE_OPENAI_EMBEDDING_DEPLOYMENT"],
    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
    api_key=os.environ["AZURE_OPENAI_API_KEY"],
    api_version=os.environ["AZURE_OPENAI_API_VERSION"],
)

endpoint = os.environ["AZURE_SEARCH_ENDPOINT"]
key = os.environ["AZURE_SEARCH_KEY"]
index_name = os.getenv("SEARCH_INDEX_NAME", "log-index")

# Vector-only ê¸°ë³¸ (ë²„ì „ ì°¨ì´ í¡ìˆ˜)
try:
    vectorstore = AzureSearch(
        azure_search_endpoint=endpoint,
        azure_search_key=key,
        index_name=index_name,
        embedding=emb,
        vector_field="content_vector",
        text_field="content",
    )
except TypeError:
    vectorstore = AzureSearch(
        azure_search_endpoint=endpoint,
        azure_search_key=key,
        index_name=index_name,
        embedding_function=emb.embed_query,
        vector_field="content_vector",
        text_field="content",
    )

st.set_page_config(page_title="ë¡œê·¸ ë¶„ì„ ì±—ë´‡", page_icon="ğŸªµ")
st.title("ğŸªµ ë¡œê·¸ ë¶„ì„ ì±—ë´‡")

with st.sidebar:
    st.subheader("ê²€ìƒ‰ ì˜µì…˜")
    file_filter = st.text_input("íŒŒì¼ëª… í•„í„° (ì˜ˆ: 2025-09-23.log)", "")
    mode = st.selectbox("ê²€ìƒ‰ ëª¨ë“œ", ["Vector", "Hybrid(ê°€ëŠ¥ ì‹œ)"], index=0)
    k = st.slider("Top-K", min_value=1, max_value=10, value=4)
    st.markdown("---")
    st.caption("LangSmith/LangFuse, AppInsightsë¡œ ìš”ì²­/ì‘ë‹µì´ ì¶”ì ë©ë‹ˆë‹¤.")

query = st.text_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”?", placeholder="ì˜ˆ: 9/23 ë¡œê·¸ì—ì„œ Exception/Timeout ìš”ì•½í•´ì¤˜")

if st.button("ì§ˆì˜í•˜ê¸°", type="primary", disabled=not query):
    t0 = time.time()
    # Telemetry span ì‹œì‘
    with tracer.start_as_current_span("qa_query") as span:
        span.set_attribute("query", query)
        span.set_attribute("mode", mode)
        if file_filter:
            span.set_attribute("filter.filename", file_filter)

        # LangFuse/LangSmith trace
        lf_trace, ls_run = None, None
        if lf:
            with suppress(Exception):
                lf_trace = lf.trace(name="qa", input={"query": query, "mode": mode, "filename": file_filter or None})
        if ls_client:
            with suppress(Exception):
                ls_run = ls_client.create_run(name="streamlit_qa", run_type="chain",
                                              inputs={"query": query, "mode": mode, "filename": file_filter or None},
                                              tags=["qa"])

        try:
            # retrieverì— file_filter ì ìš© (ì˜µì…˜1 í•µì‹¬ ë¶€ë¶„)
            # --- retriever ìƒì„± ---
            search_kwargs = {}
            if file_filter:
                search_kwargs["filter"] = {"filename": file_filter}
            retriever = vectorstore.as_retriever(search_kwargs=search_kwargs)

            # --- hybrid ê²€ìƒ‰ (í•„ìš”í•œ ê²½ìš°) ---
            sources = []
            if mode.startswith("Hybrid"):
                with suppress(Exception):
                    sources = vectorstore.hybrid_search(query, k=k)  # âœ… këŠ” ì—¬ê¸°ì„œë§Œ!


            # QA ì²´ì¸
            qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True, chain_type="stuff")
            result = qa.invoke({"query": query})
            answer = result["result"]
            used = result.get("source_documents", [])

            # ì¶œë ¥
            st.subheader("ë‹µë³€")
            st.write(answer)
            st.subheader("ì¶œì²˜")
            for i, d in enumerate(used[:k], 1):
                fname = getattr(d, "metadata", {}).get("filename") if hasattr(d, "metadata") else None
                snippet = d.page_content[:200].replace("\n", " ")
                st.write(f"[{i}] **{fname}** â€” {snippet}...")

            # í…”ë ˆë©”íŠ¸ë¦¬ ê¸°ë¡
            span.set_attribute("latency_ms", int((time.time() - t0) * 1000))
            if lf_trace:
                with suppress(Exception):
                    lf_trace.update(output={"answer": answer[:300]})
                    lf_trace.event(name="qa_sources", metadata={"count": len(used), "filenames": [getattr(d, 'metadata', {}).get('filename') for d in used]})
                    lf.flush()
            if ls_run:
                with suppress(Exception):
                    ls_client.update_run(run_id=ls_run.id, outputs={"answer": answer[:300]}, end_time=None)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")
            span.record_exception(e)
            if lf_trace:
                with suppress(Exception):
                    lf_trace.event(name="qa_error", level="error", metadata={"error": str(e)})
                    lf.flush()
            if ls_run:
                with suppress(Exception):
                    ls_client.update_run(run_id=ls_run.id, error=str(e))
